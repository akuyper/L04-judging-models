---
title: "L04 Judging Models"
subtitle: "Data Science 2 with R (STAT 301-2)"
author: "YOUR NAME"

format:
  html:
    toc: true
    toc-depth: 4
    embed-resources: true
    code-fold: show
    link-external-newwindow: true
    
execute:
  warning: false
  
from: markdown+emoji   
---

::: {.callout-important collapse="true"}
## When completing your lab write-up

Students must work in an R project connected to a GitHub repository for each lab. The repository should be well organized and it should have all relevant files. Within the project/repo there should be

-   an appropriately named qmd file and
-   the associated rendered html file (see canvas for naming convention);
-   there should be multiple R scripts (appropriately named and ordered) completing the work in the labs;
-   students should create/update README files in GitHub accordingly;

Data processing and model fitting, especially model fitting, can take significant computational time. Re-running time consuming processes when rendering a document is extremely inefficient and must be avoided.

This means students should practice writing these processes in scripts, saving results, and then loading them correctly when needed in their lab write-ups. It sometimes will be necessary to display code (show it, but don't run it) or even hide some code chunks when providing answers in the lab write-up.

Remember to **make this document your own!** Meaning you should play with the layout and think about removing unnecessary sections or items (like this callout box block). Conversely you may end up adding sections or items. Make sure all of your solutions are clearly identified and communicated. 
:::

::: {.callout-important collapse="true"}
## Load Package(s) & Setting a Seed

Recall that it is best practice to load your packages towards the top of your document.

Now that we are performing steps that involve randomness (for example data splitting and fitting random forest models) it is best practice to set a seed for the pseudo random algorithms.

**Why?** Because it ensures our random steps are reproducible which has all kinds of practical benefits. Kind of mind blowing to replicate things that are supposed to be random!

Students should set the seed directly before any random process and make a comment/note at the top of any R script that alerts potential users that a random process is being used.
:::

::: {.callout-tip icon="false"}
## Github Repo Link

To link to your github **repo**sitory, appropriately edit the example link below. Meaning replace `https://your-github-repo-url` with your github repo url. Suggest verifying the link works before submitting.

[https://your-github-repo-url](https://your-github-repo-url)
:::

## Overview

The goals for this lab are to continue using the `recipes` package to preform feature engineering, use the `yardstick` package to assess and compare models, and to train binary classification models.

This lab covers material up to and including [9. Judging model effectiveness](https://www.tmwr.org/performance.html) from [Tidy Modeling with R](https://www.tmwr.org/). We are tying up the bare minimum needed to build predictive models using `tidymodels`.

## Exercises

### Exercise 1

For this exercise, we will be working with a data set from the UCI (University of California, Irvine) Machine Learning repository ([see website here](http://archive.ics.uci.edu/ml/datasets/Abalone)). The full data set consists of 4,177 observations of abalone in Tasmania. (Fun fact: [Tasmania](https://en.wikipedia.org/wiki/Tasmania "Tasmania") supplies about 25% of the yearly world abalone harvest.)

![Inside of an abalone shell](images/AbaloneInside.jpg){#fig-abolone width="252"}

The age of an abalone is typically determined by cutting the shell open and counting the number of rings with a microscope. The purpose of this data set is to determine whether abalone age (**number of rings + 1.5**) can be accurately predicted using other, easier-to-obtain information about the abalone.

The full abalone data set is located in the `data/` subdirectory. Read it into *R* as a tibble. Take a moment to read through the codebook (`abalone_codebook.txt`) and familiarize yourself with the variable definitions.

::: {.callout-note icon="false"}
## Prediction goal

Our goal is to predict abalone age, which is calculated as the number of rings plus 1.5. Notice there currently is no `age` variable in the data set.
:::

#### Task 1

Add `age`, the target variable, to the data set. Describe the distribution of `age`.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 2

Split the abalone data into a training set and a testing set. Use stratified sampling. You should decide on appropriate percentages for splitting the data. This should be done in the `exercise_1/1_initial_setup.R` script, but remember to provide display code for graders.  

*Remember to set a seed so work is reproducible.*

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 3

Using the **training** data, create a recipe appropriate for fitting linear models (for example Ordinary Least Squares & Regularized/Penalized Regression). We want to predict the outcome variable, `age`, with all other predictor variables. Note that you should not include `rings` to predict `age`. Explain why you shouldn't use `rings` to predict `age`.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

Steps for the recipe:

-  dummy code any categorical predictors
-  create interactions between
    -   `type` and `shucked_weight`,
    -   `longest_shell` and `diameter`,
    -   `shucked_weight` and `shell_weight`
-  center all predictors, and
-  scale all predictors.

This recipe should be built in `exercise_1/2_recipes.R`, but remember to provide display code for graders.  

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 4

Define/create a workflow called `lm_wflow` for training a linear regression model using the `"lm"` engine and the pre-processing recipe defined in the previous task.

Basic steps to set up workflow:

1.  set up an empty workflow,
2.  add the model specification (provide in R script), and
3.  add the recipe created in Task 3.

After setting up the workflow, use `fit()` to train your workflow. Save these results. 

This work should be completed in `exercise_1/3_fit_lm.R`, but remember to provide display code for graders.  

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 5

Now you want to assess your model's performance on several metrics. To do this, use the `yardstick` package:

1.  Create a metric set that includes *R^2^*, RMSE (root mean squared error), and MAE (mean absolute error).
2.  Use `predict()` and `bind_cols()` to create a tibble of your model's predicted values for the testing data along with the actual observed `age`s (these are needed to assess your model's performance).
3.  Finally, apply your metric set to the tibble, report the results, and provide an interpretation of each of the values --- MAE and RMSE are interpreted similarly while *R^2^* has a different interpretation.

This work should be completed in `exercise_1/4_model_analysis.R`. Remember to provide grader with appropriate demonstration of work and output. 

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

::: callout-note

## Useful plot

For regression problems, it can be useful to create a plot of the predicted values verses the true values --- give it a try if you want (not required).

:::

#### Task 6

We've now completed a *basic* example of statistical/machine learning using ordinary linear regression. But what if ordinary linear regression isn't the best method to use? Maybe regularized/penalized regression (like lasso or ridge) or a tree-based method would work better. Let's try a few more models:


::: {.callout-caution collapse="true" icon="false"}
## Lasso Regression 

Define, train, and assess a lasso model with penalty 0.03 starting with the same recipe used by the ordinary linear regression model.

Use `exercise_1/3_fit_lasso.R` to appropriately define and train a workflow --- the model specification is provided.

The model assessment should be completed in `exercise_1/4_model_analysis.R`. 
:::

::: {.callout-caution collapse="true" icon="false"}
## Ridge Regression

Define, train, and assess a ridge model with penalty 0.03 starting with the same recipe used by the ordinary linear regression model.

Use `exercise_1/3_fit_ridge.R` to appropriately define and train a workflow --- the model specification is provided.

The model assessment should be completed in `exercise_1/4_model_analysis.R`. 
:::

::: {.callout-caution collapse="true" icon="false"}
## Random Forest

Define, train, and assess a random forest model with the number of sampled variables to split on at each node set to 4 (`mtry = 4`) and the number or trees to grow set to 1,000 (`trees = 1000`).  

While the we could use the recipe previously used for fitting linear models, it would be sub-optimal pre-processing for a tree-based model like random forest. 

::: {.callout-note collapse="true" icon="false"}

## Note on tree-based pre-processing/recipes

Tree-based methods naturally search out interactions, meaning that we typically don't need to specify any interactions (of course, there are exceptions). Tree-based methods typically work better using one-hot encoding instead of traditional dummy coding; this also has to do with the fact that they are empirically driven models, not mechanistic.
:::

Let's create a different recipe for the random forest model. Similarly to the other recipe, we will predict `age` with all other predictor variables --- should not include `rings` to predict `age`.

Steps for the recipe:

-  one-hot encode any categorical predictors
-  center all predictors, and
-  scale all predictors.

The recipe should be added to `exercsie_1/2_recipes.R`.

Use `exercise_1/3_fit_rf.R` to appropriately define and train a workflow --- the model specification is provided.

The model assessment should be completed in `exercise_1/4_model_analysis.R`. 

:::

No display code is required for this task because. Only need a confirmation that it has been completed. The output in the next task will verify that this has been done correctly. 

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 7

Provide the performance assessment metrics for each of the 4 models in one table. After considering this information, which model do you think is best? Why?

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

### Exercise 2

::: {.callout-important}

Unlike Exercise 1, there are no explicit instruction on where to place your code and no explicit instructions about what to display to graders. 

Students are expected to figure this out.

Students will need to organize their work and present sufficient evidence for graders --- use exercise one for guidance. Some initial R scripts template are provided to get students started.

:::

For this exercise, we will be working with part of a [Kaggle data set](https://www.kaggle.com/c/titanic/overview) that was the subject of a machine learning competition and is often used for practicing ML models.

![RMS Titanic departing Southampton on April 10, 1912.](images/RMS_Titanic.jpg){fig-align="center" width="363" #fig-titanic}

::: {.callout-note icon="false"}
## Prediction goal

The goal is classification; specifically, to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).
:::

#### Task 1

Load the data from `data/titanic.csv` into *R* and familiarize yourself with the variables by reviewing the codebook (`data/titanic_codebook.csv`).

Notice that `survived` and `pclass` should be changed to factors. When changing `survived` to a factor, you should reorder the factor so that `"Yes"` is the first level.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 2

Using the full data set, explore/describe the distribution of the outcome variable `survived`.

**Only do this for the target variable.**

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 3

Split the data! Use stratified sampling. You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations. Perform a skim of the training data and note any potential issues such as missingness.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

Why is it a good idea to use stratified sampling for this data?

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 4

Looking ahead, we plan to train two random forest models and a logistic regression model for this problem. We begin by setting up recipes for each of these approaches.

::: {.callout-caution collapse="true" icon="false"}
## Logistic Regression Recipe

Using the training data, create and store a recipe setting `survived` as the outcome and using the following predictors: ticket class, sex, age, number of siblings or spouses aboard, number of parents or children aboard, and passenger fare.

Recall that there were missing values for `age`. To deal with this, add an imputation step using `step_impute_linear()`. Next, use `step_dummy()` to dummy encode categorical predictors. Finally, include interactions between:

-   Sex and passenger fare, and
-   Age and passenger fare.

:::

::: {.callout-caution collapse="true" icon="false"}
#### Tree-Based Recipe

Using the training data, create and store a recipe setting `survived` as the outcome and using the following predictors: ticket class, sex, age, number of siblings or spouses aboard, number of parents or children aboard, and passenger fare.

Recall that there were missing values for `age`. To deal with this, add an imputation step using `step_impute_linear()`. Next, use `step_dummy()` to one-hot encode categorical predictors.

::: {.callout-note collapse="true" icon="false"}
## Note on tree-based pre-processing/recipes

Tree-based methods naturally search out interactions, meaning that we typically don't need to specify any interactions (of course, there are exceptions). Tree-based methods typically work better using one-hot encoding instead of traditional dummy coding; this also has to do with the fact that they are empirically driven models, not mechanistic.
:::

:::

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::


#### Task 5

Create a workflow for fitting a **logistic regression** model for classification using the `"glm"` engine. Add your specified model and the appropriate recipe.

Now use `fit()` to train your workflow.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 6

**Repeat Task 5**, but this time specify a random forest model for classification using the `"ranger"` engine and the appropriate recipe. *Don't specify values for tuning parameters manually;* allow the function(s) to use the default values.

Using `?rand_forest`, read the function documentation to find out what default values `ranger` uses for `mtry`, `trees`, and `min_n`. What are the defaults in this case?

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 7

**Repeat Task 6**, but this time choose values that you think are reasonable for each of the three tuning parameters (`mtry`, `trees`, and `min_n`).

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 8

Now you've trained three different models/workflows to the training data:

1.  A logistic regression model
2.  A random forest model with default tuning parameters
3.  A random forest model with custom tuning parameters

Use `predict()` and `bind_cols()` to generate predictions using each of these 3 models and your testing data. Then use the **accuracy** metric to assess the performance of each of the three models.

Which model makes the best predictions? How do you know?

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 9

For the model identified in Task 8, create a confusion matrix using the testing data.

Explain what this is in your own words. Interpret the numbers in each category.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 10

For the model identified in Task 8, use `predict()` and `bind_cols()` to create a tibble of predicted class probabilities and actual true outcomes. Note that this will require using the `type` argument of `predict()`. You should be using the testing data.

Explain what these class probabilities are in your own words.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 11

For the model identified in Task 8, use `roc_curve()` and `autoplot()` to create a receiver operating characteristic (ROC) curve.

Use `roc_auc()` to calculate the area under the ROC curve.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 12

The area under the ROC curve is a measure of how well the model predictions are able to separate the data being tested into classes/groups. [(See here for a more detailed explanation)](http://gim.unmc.edu/dxtests/roc3.htm).

Interpret the AUC for your model.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::
